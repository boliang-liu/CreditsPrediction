{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPBlazZYH179sH0yWY+YP0M",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/boliang-liu/CreditsPrediction/blob/main/Credits_prediction_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hk5eIis_a5m"
      },
      "source": [
        "import pandas as pd\n",
        "from   category_encoders          import *\n",
        "import numpy as np\n",
        "from   sklearn.compose            import *\n",
        "from   sklearn.impute             import *\n",
        "from   sklearn.metrics            import mean_squared_error, mean_absolute_error, r2_score\n",
        "from   sklearn.pipeline           import Pipeline\n",
        "from   sklearn.preprocessing      import *\n",
        "from   sklearn.linear_model       import *\n",
        "from   sklearn.model_selection    import *\n",
        "from   sklearn.decomposition      import PCA\n",
        "from   sklearn.svm                import SVC\n",
        "from   sklearn.ensemble           import ExtraTreesRegressor, RandomForestRegressor\n",
        "from   sklearn.base               import BaseEstimator"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEjUl5KqKwH0"
      },
      "source": [
        "credit = pd.read_csv('https://raw.githubusercontent.com/boliang-liu/CreditsPrediction/main/BankChurners.csv')"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRJN9yscnnbW"
      },
      "source": [
        "Load the dataset from Github repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnURKUR9LLEb"
      },
      "source": [
        "data = credit[['Customer_Age', 'Gender',\n",
        "       'Dependent_count', 'Education_Level', 'Marital_Status',\n",
        "       'Income_Category', 'Card_Category', 'Months_on_book',\n",
        "       'Total_Relationship_Count', 'Months_Inactive_12_mon',\n",
        "       'Contacts_Count_12_mon', 'Total_Revolving_Bal',\n",
        "       'Avg_Open_To_Buy', 'Total_Amt_Chng_Q4_Q1', 'Total_Trans_Amt',\n",
        "       'Total_Trans_Ct', 'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio',\n",
        "       'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',\n",
        "       'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2']]"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enbORZFWqNrM"
      },
      "source": [
        "Select features we need to use, drop meaningless columns to this project like ID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-OihxJ8LT4E"
      },
      "source": [
        "target = credit['Credit_Limit']\n",
        "target = target.values.ravel()"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbCIekcSq5Ws"
      },
      "source": [
        "Select the target column 'Credit_Limit' and change the type for fitting models later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E92bqxUHLVbV"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data, \n",
        "                                                    target, \n",
        "                                                    test_size=0.2)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV_j0zFJrOjW"
      },
      "source": [
        "Split the dataset to train set and test set, test size is 20%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQgYw_D-LXcB"
      },
      "source": [
        "categorical_columns = (X_train.dtypes == object)\n",
        "continuous_columns  = (X_train.dtypes != object)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AkvspeCraeZ"
      },
      "source": [
        "Identify the types of columns, continous or categorical. It's useful for transforming columns later "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIh3qNXYLZCL"
      },
      "source": [
        "class DummyEstimator(BaseEstimator):\n",
        "    \"Pass through class, methods are present but do nothing.\"\n",
        "    def fit(self): pass\n",
        "    def score(self): pass"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIEzRRrOr1F8"
      },
      "source": [
        "Custom a class for later RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBIPJaFrLaRd"
      },
      "source": [
        "con_pipe = Pipeline([('scalar', MaxAbsScaler()),\n",
        "                     ('imputer', SimpleImputer(missing_values=np.nan, strategy='median', add_indicator=True))\n",
        "                     ])\n",
        "\n",
        "cat_pipe = Pipeline([('ohe', OneHotEncoder(handle_unknown='ignore')),\n",
        "                     ('imputer', SimpleImputer(strategy='most_frequent', add_indicator=True))])\n",
        "\n",
        "preprocessing = ColumnTransformer([('categorical', cat_pipe,  categorical_columns),\n",
        "                                   ('continuous',  con_pipe,  continuous_columns),\n",
        "                                   ])\n",
        "pipe = Pipeline([('preprocessing', preprocessing),\n",
        "                 ('pca', PCA(n_components=15)),\n",
        "                 ('clf',DummyEstimator())\n",
        "                ])"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R56Kvostr-Ft"
      },
      "source": [
        "Standardize continous columns and impute missing values to the median of the column. OneHotEncode categorical columns and imputer missing values to the most frequent category of the column. Construct a columns transformer containing categorial and continous, then construct a pipeline containing columns tranformer, pca, and DummyEstimator. There's 20 features in total but some features are not important. To improve generality, we set n_components=15 to choose 15 important features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fY29h53sLc1_",
        "outputId": "810b9fa7-fba2-4376-d890-7c3e9937a126"
      },
      "source": [
        "search_space = [{'clf': [RandomForestRegressor()],\n",
        "                 'clf__n_estimators': np.arange(100, 1000, 150), # decides how many trees\n",
        "                 'clf__max_features': ['log2','sqrt'], # decides how many features in each tree\n",
        "                 'clf__max_depth' : np.arange(15,25,1), # decides how deep in each tree\n",
        "                 'clf__min_samples_leaf': np.arange(1,10,1), # decides hwo many samples at minimum in each leaf\n",
        "                 'clf__bootstrap': [True, False] # decides if using bootstrap technique\n",
        "                },\n",
        "                \n",
        "                {'clf': [SVC()], \n",
        "                 'clf__C': np.logspace(0.1, 1000, 5), # decides the best C value\n",
        "                 'clf__gamma': np.logspace(0.0001,1,5), # decides the best gamma\n",
        "                 'clf__kernel':['rbf','poly'], # decides kernal type\n",
        "                 'clf__class_weight': ['balanced',None] # decides the type of weighted class\n",
        "                },\n",
        "               \n",
        "                {'clf': [RidgeCV()], \n",
        "                 'clf__normalize': [False, True], # decides if doing normalization\n",
        "                 'clf__alpha_per_target' : [False, True] # decides if using alpha in per target\n",
        "                },\n",
        "                \n",
        "                {'clf': [LassoCV()], \n",
        "                 'clf__eps': np.arange(0.0005, 0.01, 0.0005), # decides the best epsilon\n",
        "                 'clf__normalize': [False, True], # decides if doing normalization\n",
        "                 'clf__max_iter': np.arange(1000,5000,1000), # decides the maximum times of iteration\n",
        "                 'clf__n_alphas': np.arange(100,500,100) # decides the best n_alphas\n",
        "                },\n",
        "                \n",
        "                {'clf': [BayesianRidge()], \n",
        "                 'clf__normalize': [False, True], # decides if doing normalization\n",
        "                 'clf__n_iter': np.arange(100, 1000, 100) # decides the times of iteration\n",
        "                },\n",
        "                \n",
        "                {\n",
        "                 'clf': [HuberRegressor()],\n",
        "                 'clf__alpha': np.arange(0.0001, 0.001, 0.0001), # decides the best alpha\n",
        "                 'clf__max_iter': np.arange(100,1000,100), # decides the times of iteration\n",
        "                 'clf__epsilon': np.arange(1,2,0.1) # decides the best epsilon\n",
        "                },\n",
        "                \n",
        "                {\n",
        "                 'clf': [ExtraTreesRegressor()], \n",
        "                 'clf__max_features': ['log2','sqrt'], # decides how many features in each tree\n",
        "                 'clf__max_depth' : np.arange(15,25,1), # decides how deep in each tree\n",
        "                 'clf__n_estimators': np.arange(100, 1000, 150), # decides how many trees\n",
        "                 'clf__min_samples_leaf': np.arange(1,10,1), # decides hwo many samples at minimum in each leaf\n",
        "                 'clf__bootstrap': [True, False] # decides if using bootstrap technique\n",
        "                }\n",
        "                 ]\n",
        "\n",
        "clf_algos_rand = RandomizedSearchCV(estimator=pipe,\n",
        "                                    param_distributions=search_space, \n",
        "                                    n_iter=50,\n",
        "                                    cv=5, \n",
        "                                    n_jobs=-1,\n",
        "                                    verbose=10,\n",
        "                                    scoring='neg_root_mean_squared_error')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/function_base.py:265: RuntimeWarning: overflow encountered in power\n",
            "  return _nx.power(base, y)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgjdBf_stVuj"
      },
      "source": [
        "Construct a search space of 7 different models and various hyperparameters. Use RandomizedSearchCV to search the best model and hyperparameters using MSE as scoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e5FWlAZLe9-",
        "outputId": "8426a652-a356-4a73-a4ec-f888a281d7b9"
      },
      "source": [
        "for i in range(5):\n",
        "\n",
        "    best_model = clf_algos_rand.fit(X_train, y_train)\n",
        "\n",
        "    print(best_model.best_estimator_.get_params()['clf'], end='\\n')\n",
        "    print(best_model.best_score_, end='\\n')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    6.3s\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   11.2s\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   23.8s\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:   57.5s\n",
            "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:  1.4min\n",
            "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:  2.9min\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  3.5min\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  4.0min\n",
            "[Parallel(n_jobs=-1)]: Done  57 tasks      | elapsed:  5.1min\n",
            "/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "[Parallel(n_jobs=-1)]: Done  68 tasks      | elapsed:  7.3min\n",
            "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:  7.5min\n",
            "[Parallel(n_jobs=-1)]: Done  94 tasks      | elapsed:  7.8min\n",
            "[Parallel(n_jobs=-1)]: Done 109 tasks      | elapsed:  8.7min\n",
            "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:  9.6min\n",
            "[Parallel(n_jobs=-1)]: Done 141 tasks      | elapsed: 10.2min\n",
            "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed: 11.3min\n",
            "[Parallel(n_jobs=-1)]: Done 177 tasks      | elapsed: 12.1min\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed: 12.7min\n",
            "[Parallel(n_jobs=-1)]: Done 217 tasks      | elapsed: 13.1min\n",
            "[Parallel(n_jobs=-1)]: Done 238 tasks      | elapsed: 14.6min\n",
            "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed: 15.0min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "RandomForestRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
            "                      max_depth=19, max_features='log2', max_leaf_nodes=None,\n",
            "                      max_samples=None, min_impurity_decrease=0.0,\n",
            "                      min_impurity_split=None, min_samples_leaf=1,\n",
            "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
            "                      n_estimators=850, n_jobs=None, oob_score=False,\n",
            "                      random_state=None, verbose=0, warm_start=False)\n",
            "-1771.6695369482761\n",
            "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   19.0s\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   38.4s\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  1.3min\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:  1.4min\n",
            "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:  1.7min\n",
            "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:  1.9min\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  2.6min\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  3.1min\n",
            "[Parallel(n_jobs=-1)]: Done  57 tasks      | elapsed:  3.9min\n",
            "[Parallel(n_jobs=-1)]: Done  68 tasks      | elapsed:  4.3min\n",
            "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:  5.0min\n",
            "[Parallel(n_jobs=-1)]: Done  94 tasks      | elapsed:  7.3min\n",
            "[Parallel(n_jobs=-1)]: Done 109 tasks      | elapsed:  8.4min\n",
            "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed: 10.1min\n",
            "[Parallel(n_jobs=-1)]: Done 141 tasks      | elapsed: 10.6min\n",
            "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed: 11.5min\n",
            "[Parallel(n_jobs=-1)]: Done 177 tasks      | elapsed: 12.3min\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed: 13.3min\n",
            "[Parallel(n_jobs=-1)]: Done 217 tasks      | elapsed: 14.5min\n",
            "[Parallel(n_jobs=-1)]: Done 238 tasks      | elapsed: 16.3min\n",
            "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed: 17.1min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "RandomForestRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
            "                      max_depth=20, max_features='log2', max_leaf_nodes=None,\n",
            "                      max_samples=None, min_impurity_decrease=0.0,\n",
            "                      min_impurity_split=None, min_samples_leaf=1,\n",
            "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
            "                      n_estimators=250, n_jobs=None, oob_score=False,\n",
            "                      random_state=None, verbose=0, warm_start=False)\n",
            "-1765.3591850968237\n",
            "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   10.3s\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   20.8s\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   52.5s\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:  1.2min\n",
            "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:  1.4min\n",
            "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:  2.0min\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  2.1min\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  2.3min\n",
            "[Parallel(n_jobs=-1)]: Done  57 tasks      | elapsed:  3.8min\n",
            "[Parallel(n_jobs=-1)]: Done  68 tasks      | elapsed:  4.4min\n",
            "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:  5.0min\n",
            "[Parallel(n_jobs=-1)]: Done  94 tasks      | elapsed:  6.5min\n",
            "[Parallel(n_jobs=-1)]: Done 109 tasks      | elapsed:  6.8min\n",
            "/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:  9.6min\n",
            "[Parallel(n_jobs=-1)]: Done 141 tasks      | elapsed: 10.8min\n",
            "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed: 11.9min\n",
            "[Parallel(n_jobs=-1)]: Done 177 tasks      | elapsed: 13.2min\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed: 14.0min\n",
            "[Parallel(n_jobs=-1)]: Done 217 tasks      | elapsed: 14.7min\n",
            "[Parallel(n_jobs=-1)]: Done 238 tasks      | elapsed: 17.6min\n",
            "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed: 19.0min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "RandomForestRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
            "                      max_depth=20, max_features='log2', max_leaf_nodes=None,\n",
            "                      max_samples=None, min_impurity_decrease=0.0,\n",
            "                      min_impurity_split=None, min_samples_leaf=1,\n",
            "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
            "                      n_estimators=700, n_jobs=None, oob_score=False,\n",
            "                      random_state=None, verbose=0, warm_start=False)\n",
            "-1766.55209906544\n",
            "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   17.2s\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   35.0s\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   52.0s\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:   54.6s\n",
            "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:  1.9min\n",
            "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:  3.0min\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  3.9min\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  5.7min\n",
            "[Parallel(n_jobs=-1)]: Done  57 tasks      | elapsed:  5.8min\n",
            "[Parallel(n_jobs=-1)]: Done  68 tasks      | elapsed:  6.1min\n",
            "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:  6.4min\n",
            "[Parallel(n_jobs=-1)]: Done  94 tasks      | elapsed:  6.7min\n",
            "[Parallel(n_jobs=-1)]: Done 109 tasks      | elapsed:  8.5min\n",
            "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:  9.9min\n",
            "[Parallel(n_jobs=-1)]: Done 141 tasks      | elapsed: 10.1min\n",
            "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed: 10.9min\n",
            "[Parallel(n_jobs=-1)]: Done 177 tasks      | elapsed: 12.2min\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed: 12.9min\n",
            "[Parallel(n_jobs=-1)]: Done 217 tasks      | elapsed: 13.1min\n",
            "[Parallel(n_jobs=-1)]: Done 238 tasks      | elapsed: 16.0min\n",
            "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed: 16.3min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "RandomForestRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
            "                      max_depth=23, max_features='sqrt', max_leaf_nodes=None,\n",
            "                      max_samples=None, min_impurity_decrease=0.0,\n",
            "                      min_impurity_split=None, min_samples_leaf=2,\n",
            "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
            "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
            "                      random_state=None, verbose=0, warm_start=False)\n",
            "-1814.8345727824217\n",
            "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    4.6s\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    9.2s\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   22.5s\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:  1.4min\n",
            "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:  1.5min\n",
            "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:  2.2min\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  4.7min\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  5.3min\n",
            "[Parallel(n_jobs=-1)]: Done  57 tasks      | elapsed:  6.7min\n",
            "[Parallel(n_jobs=-1)]: Done  68 tasks      | elapsed:  7.2min\n",
            "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:  7.5min\n",
            "[Parallel(n_jobs=-1)]: Done  94 tasks      | elapsed:  8.0min\n",
            "[Parallel(n_jobs=-1)]: Done 109 tasks      | elapsed:  8.8min\n",
            "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed: 10.3min\n",
            "[Parallel(n_jobs=-1)]: Done 141 tasks      | elapsed: 11.2min\n",
            "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed: 11.8min\n",
            "[Parallel(n_jobs=-1)]: Done 177 tasks      | elapsed: 12.5min\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed: 12.8min\n",
            "[Parallel(n_jobs=-1)]: Done 217 tasks      | elapsed: 14.6min\n",
            "[Parallel(n_jobs=-1)]: Done 238 tasks      | elapsed: 15.7min\n",
            "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed: 16.0min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "RandomForestRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
            "                      max_depth=17, max_features='log2', max_leaf_nodes=None,\n",
            "                      max_samples=None, min_impurity_decrease=0.0,\n",
            "                      min_impurity_split=None, min_samples_leaf=2,\n",
            "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
            "                      n_estimators=550, n_jobs=None, oob_score=False,\n",
            "                      random_state=None, verbose=0, warm_start=False)\n",
            "-1847.4984585662569\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYPIkbryt4K-"
      },
      "source": [
        "Run 5 times to look at 5 best models and hyperparameters. By comparing 5 MSE values to choose the final best one model and hyperparameters. The final best model in RandomForestRegressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNN3aDBAQIiu",
        "outputId": "08fb463d-e474-4ac9-99e4-44c8894d4458"
      },
      "source": [
        "RandomForestRegressor().get_params()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bootstrap': True,\n",
              " 'ccp_alpha': 0.0,\n",
              " 'criterion': 'mse',\n",
              " 'max_depth': None,\n",
              " 'max_features': 'auto',\n",
              " 'max_leaf_nodes': None,\n",
              " 'max_samples': None,\n",
              " 'min_impurity_decrease': 0.0,\n",
              " 'min_impurity_split': None,\n",
              " 'min_samples_leaf': 1,\n",
              " 'min_samples_split': 2,\n",
              " 'min_weight_fraction_leaf': 0.0,\n",
              " 'n_estimators': 100,\n",
              " 'n_jobs': None,\n",
              " 'oob_score': False,\n",
              " 'random_state': None,\n",
              " 'verbose': 0,\n",
              " 'warm_start': False}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KymqZQJ4uXQ2"
      },
      "source": [
        "Check the default hyperparameters of RandomForestRegressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJ2BhQKR0x7F"
      },
      "source": [
        "params={'bootstrap': False,\n",
        " 'ccp_alpha': 0.0,\n",
        " 'criterion': 'mse',\n",
        " 'max_depth': 20,\n",
        " 'max_features': 'log2',\n",
        " 'max_leaf_nodes': None,\n",
        " 'max_samples': None,\n",
        " 'min_impurity_decrease': 0.0,\n",
        " 'min_impurity_split': None,\n",
        " 'min_samples_leaf': 1,\n",
        " 'min_samples_split': 2,\n",
        " 'min_weight_fraction_leaf': 0.0,\n",
        " 'n_estimators': 250,\n",
        " 'n_jobs': None,\n",
        " 'oob_score': False,\n",
        " 'random_state': None,\n",
        " 'verbose': 0,\n",
        " 'warm_start': False}"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElxidiW3udHJ"
      },
      "source": [
        "Change the default hyperparameters into choosen best hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6dbWv4F1Kv2"
      },
      "source": [
        "pipe = Pipeline([('preprocessing', preprocessing),\n",
        "                 ('pca', PCA(n_components=15)),\n",
        "                 ('reg',RandomForestRegressor(**params))\n",
        "                ])"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VF_04DENuqQq"
      },
      "source": [
        "Construct the final pipeline by adding the best model and hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUsMSEa91Q-d",
        "outputId": "2e65244c-27ef-45c8-e006-872a10dee859"
      },
      "source": [
        "pipe.fit(X_train, y_train)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('preprocessing',\n",
              "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
              "                                   sparse_threshold=0.3,\n",
              "                                   transformer_weights=None,\n",
              "                                   transformers=[('categorical',\n",
              "                                                  Pipeline(memory=None,\n",
              "                                                           steps=[('ohe',\n",
              "                                                                   OneHotEncoder(categories='auto',\n",
              "                                                                                 drop=None,\n",
              "                                                                                 dtype=<class 'numpy.float64'>,\n",
              "                                                                                 handle_unknown='ignore',\n",
              "                                                                                 sparse=True)),\n",
              "                                                                  ('imputer',\n",
              "                                                                   SimpleImputer(add_indicator=...\n",
              "                 RandomForestRegressor(bootstrap=False, ccp_alpha=0.0,\n",
              "                                       criterion='mse', max_depth=20,\n",
              "                                       max_features='log2', max_leaf_nodes=None,\n",
              "                                       max_samples=None,\n",
              "                                       min_impurity_decrease=0.0,\n",
              "                                       min_impurity_split=None,\n",
              "                                       min_samples_leaf=1, min_samples_split=2,\n",
              "                                       min_weight_fraction_leaf=0.0,\n",
              "                                       n_estimators=250, n_jobs=None,\n",
              "                                       oob_score=False, random_state=None,\n",
              "                                       verbose=0, warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHfZMHAPu2Kt"
      },
      "source": [
        "Train the model by using train set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VkRh5Kq1TFZ"
      },
      "source": [
        "y_pred   = pipe.predict(X_test)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3yt9vYAu6aG"
      },
      "source": [
        "Use trained model to do prediction for test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwjkTrHX1VRw"
      },
      "source": [
        "mse  = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r_2 = r2_score(y_test, y_pred)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cddftmGVvAtB"
      },
      "source": [
        "Calculate MSE, MAE, R2 of between prediction and y_test. These 3 evaluation metrics are appropriate metrics for regression so they are reasonable. MSE means mean squared error regression loss between prediction and actual values. MAE means median absolute error regression loss between prediction and actual values. R2 means coefficient of determination of the prediction model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXLGBAIh2E7R",
        "outputId": "1ac5dc54-e2bd-4ff1-c5fd-9272f03bc0e4"
      },
      "source": [
        "print('mse = {mse}\\nmae = {mae}\\nr2  = {r_2}'.format(mse=mse, mae=mae, r_2=r_2))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mse = 2575826.8301345236\n",
            "mae = 1044.6272158530253\n",
            "r2  = 0.9702055308283112\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgtdARmWwY22"
      },
      "source": [
        "Print out the scores together.\n",
        "mse = 2575826.8301345236.\n",
        "mae = 1044.6272158530253.\n",
        "r2  = 0.9702055308283112.\n",
        "The mean squared error regression loss between prediction and actual values is 2575826.8301345236. The median absolute error regression loss between prediction and actual values is 1044.6272158530253. The R2 coefficient of determination of the prediction model is 0.97."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpR0L4ZP2fqP"
      },
      "source": [
        "The searched model and hyperparameters are good for predicting the credit limit. Next, I'll do grid search if possible, which is more accurate but would cost more time."
      ]
    }
  ]
}